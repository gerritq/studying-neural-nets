{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from Scratch\n",
    "\n",
    "In this notebook, I implement Andrej Karpathy's `nanogpt` version from his [lecture series](https://karpathy.ai/zero-to-hero.html).\n",
    "\n",
    "In this notebook, we code a small decoder-based language model that predicts the next token in a sequence, which is fundamentally a GPT. To train the model, we use the `tiny_shakespeare` dataset. Once the model is trained, it can produce Shakespeare-like text (if scaled up considerably).\n",
    "\n",
    "The landmark paper that proposed the transformer architecture is the core of the model. In the following, we build a transformer from scratch, train it on the `tiny_shakespeare` dataset, and then generate new Shakespearean text.\n",
    "\n",
    "This notebook covers the following concepts, for which I provide additional depth:\n",
    "\n",
    "- Self-attention\n",
    "- Multi-Head Attention\n",
    "- Residual Connection\n",
    "- Layer vs. Batch Normalisation\n",
    "- Dropout (will be done more)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset can be retrieved from: ...\n",
    "\n",
    "The total dataset has around 1 million characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "with open('/Users/gerritquaremba/Library/CloudStorage/GoogleDrive-g.quaremba@gmail.com/My Drive/08_NN_Kaparthy/ng-video-lecture/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num total chars 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "print('Num total chars', len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to get all unique characters in the corpus, which is the vocabulary size. We have in total `65` characters, including special characters such as commas, dashes, etc. We can also check which characters are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all unique chars and voc size\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size=len(chars)\n",
    "print(''.join(chars))\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to tokenize the characters into unique integer representations. We do not feed characters into a neural network, but rather integers that uniquely identify characters. We do this by mapping integers to strings, and vice versa. We also want `decoder` and `encoder` functions. A decoder takes a list of integers and decodes them into characters. An encoder takes a string and encodes it into unique numbers.\n",
    "\n",
    "Note: this is the simplest way to encode characters/words. There are multiple other methods, such as sentence or sub-word tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map string to int\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Map int to string\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encoder Function\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "# Decoder Function\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " $&\n",
      "[46, 43, 50, 50, 53, 1, 28, 46, 16]\n"
     ]
    }
   ],
   "source": [
    "# See how a decoder works\n",
    "print(decode([1, 3, 4]))\n",
    "\n",
    "# See how a encoder works\n",
    "print(encode('hello PhD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can encode entire corpus and pass it to a PyTorch tensor. The tensor will be an array of size of the length of the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "# Encode text and store in tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Show data size\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we output is the same text as we loaded above but just represented as integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation Split\n",
    "\n",
    "We will use 90% of the data for the training and the remainder for validation. With that we can check whether our model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and val data\n",
    "n = int(.9*len(data))\n",
    "train_data=data[:n]\n",
    "val_data=data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "### Block Size/Context Window\n",
    "\n",
    "We cannot input all the text at once as this is computationally too expensive. Instead, we can choose a block size of `8`, for example. However, we have to select `9` characters, as one example is defined by predicting $char_{i}$ given all previous characters $char_{j<i}$. To define the inputs and outputs, we simply use data of the size of a block for the inputs. The output is the subsequent character of a given sequence of characters, as the model learns to predict the next token in a sequence. The model looks up to the length of the defined block size (also called context window).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([18]) target 47\n",
      "input tensor([18, 47]) target 56\n",
      "input tensor([18, 47, 56]) target 57\n",
      "input tensor([18, 47, 56, 57]) target 58\n",
      "input tensor([18, 47, 56, 57, 58]) target 1\n",
      "input tensor([18, 47, 56, 57, 58,  1]) target 15\n",
      "input tensor([18, 47, 56, 57, 58,  1, 15]) target 47\n",
      "input tensor([18, 47, 56, 57, 58,  1, 15, 47]) target 58\n"
     ]
    }
   ],
   "source": [
    "# Define batch size\n",
    "block_size=8\n",
    "\n",
    "# Show how in- and outputs looks like\n",
    "x = train_data[:block_size] # First 8\n",
    "y = train_data[1:block_size+1] # Shift by one to get the next token for each x\n",
    "\n",
    "for t in range(block_size):\n",
    "     context=x[:t+1]\n",
    "     target=y[t]\n",
    "     print(f'input {context} target {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we feed inputs from the length of 1 to block size. This is by purpose so that the model learns short and longer dependecies between characters. At inference, we can thus intput short or longer sequences and the model can work with both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "\n",
    "Instead of feeding one input at a time (with 8 blocks), we will feed minibatches that consist of multiple inputs at a time. These are processed in parallel and independently by the model. A single batch is defined by the batch size and randomly picks this number of inputs from the data. To get a batch, we randomly pick `4` characters from the entire data. For each of these `4` characters, we select the subsequent `7` characters to create the context for this input. The resulting dimension of a single batch matrix is thus `4x8`, where rows represent a single input and columns represent the context. The output matrix is of the same dimension and gives for any character the subsequent one.\n",
    "\n",
    "- Batch: number of independent sequences => \"batch dimension\"\n",
    "- Block: context window => \"time dimension\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For replicability\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Set batch and block sizes\n",
    "batch_size=4\n",
    "block_size=8\n",
    "\n",
    "# Set embedding size of tokens for later\n",
    "n_embed=32\n",
    "\n",
    "# Function to get batches\n",
    "def get_batch(split):\n",
    "    '''Takes a string. Gets a single batch from the defined data. Ouputs x and y as bach in- and outputs.'''\n",
    "    \n",
    "    # Select data\n",
    "    data=train_data if split=='train' else val_data\n",
    "\n",
    "    # Get random chars of size batch_size\n",
    "    ix=torch.randint(len(data) - block_size, (batch_size,)) # why minus?\n",
    "    \n",
    "    # Get the chosen chars and their contexts, stack in one tensors\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # dim: batch_size x block_size\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #dim: batch_size x block_size\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim input torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# A single batch looks as follows\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# Shape of inputs\n",
    "print('Dim input', xb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram LM\n",
    "\n",
    "- `self_embedding_layer`: \n",
    "  This layer uses a wrapper `embedding` to create the count matrix of the bigram model. The matrix is indexed according to the integers of the string. For example, the string with integer `24` will extract the row `24` of the embedding matrix. This row then provides the counts of each of the characters that follows `24`.\n",
    "\n",
    "- Dimensions: The dimensionality of the resulting logits is as follows. Dimension 1, Batch, represents the layers of the 3D cube. Dimension 2, Time, are the rows of a layer, corresponding to the context window. Dimension 3, Channel, are the columns of a layer and provide the logits for each time.\n",
    "\n",
    "- logits: can be interpreted as the counts of the bigrams.\n",
    "\n",
    "- reshaping tensors: for the `cross_entropy()` function, PyTorch expects a specific format for multidimensional inputs, which we have with our 3-D tensor. Unlike the B,T,C dimensions of the input tensor, the function expects the channel to be on the second dimension. We thus have to reshape the input tensor, and also the output tensor to match each other. Reshaping the input to be (B*T, C) involves treating the time dimension as a batch dimension. In other words, we flatten the time dimension. Each of the characters in the input tensor is now treated as a batch row rather than a batch and time value, retaining only the channel dimension. The dimension of the logits will be `32 x 65` (`4x8` flattens the time dimension), and the dimension of the output will be `32`. The output indexes the input tensor to get the probabilities for the correct label.\n",
    "\n",
    "- Generation: the idea of generation is to pass a sequence of characters into the network, and it continues to make predictions of new characters based on this sequence. That is, the model takes a `(B,T)` input and continues adding to T, so that `(B, T+max_new_tokens)`. Here, it simply looks back at the previous character to predict the next, hence a bigram model. We pass the indexes of the sequence, convert the logits into probabilities using softmax, and sample from the probability distribution of the next character. This process continues until a specified limit is reached.\n",
    "\n",
    "- Note: the channel dimensions of the embedding and the first linear layer are different. For the embedding, the dimension `TxC` is `8x32`. For the linear layer, the dimension `TxC` is `32x65`. The resulting linear layer is of dimension `8x65` and provides the logits. Generally, by dimension `C` we mean the embedding dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram LM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "batch_size=32 # Size of samples at a time\n",
    "max_iters=10000 # Training iterations\n",
    "eval_interval=100 # Steps for evaluations during training\n",
    "eval_iters=200 # Number of intermediate evaluation batches\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# BIGRAM MODEL\n",
    "class BigramLanguageModel(nn.Module): # inherent from the nn class\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedding table, which acts as count matrix; used for directly getting logits\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # Index embeddings table with tokens\n",
    "        logits = self.token_embedding_table(idx) # Idx (B,T) indexes embd => (B,T,C); C=embed size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Logits (B,T,C); targets (B,T) => reshape to logits (B*T,C) x targets (B*T)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, new_tokens):\n",
    "        '''Generate new tokens from an input sequence. \n",
    "        As this is a bigram model, only the last Time dim is considered'''\n",
    "\n",
    "        for _ in range(new_tokens):\n",
    "\n",
    "            # Get the logits  \n",
    "            logits, loss = self(idx) # loss is ignored, no use here\n",
    "\n",
    "            # Consider last time dim only; makes (B,T,C) to (B,C) \n",
    "            logits = logits[:,-1,:] # picks last T, becomes (B,C), bc we want the latest char\n",
    "            \n",
    "            # Convert logits into probs\n",
    "            probs = F.softmax(logits, dim=1) # Row sum=1 \n",
    "\n",
    "            # Sample from each rwo\n",
    "            idx_next=torch.multinomial(probs, num_samples=1) # (B,1), as we only get one prediction\n",
    "\n",
    "            # Append idx_next to idx\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # Append across dim 1; (B, T+1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training and Validation Losses__\n",
    "\n",
    "We can estimate the training and validation losses using the function below. It computes the loss for a specified number of batches, defined by `eval_iters`, for each dataset, and then averages these to estimate the respective loss. We also set the mode of the model by calling the `eval()` and `train()` functions. This makes no difference here, but is usually done as certain layers, e.g., dropout, are not used during evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # No need for grads, more efficient\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "\n",
    "    # Set model to eval phase\n",
    "    m.eval()\n",
    "\n",
    "    # For both datasets\n",
    "    for split in ['train', 'val']:\n",
    "        losses=torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = m(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    # Set model to train phase\n",
    "    m.train()\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7943, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# For replicability\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Build model and show the resulting embedding\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Get loss without training\n",
    "logits, loss = m(xb, yb)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a prediction by inputting any sequence of the context length. For example, we can simply input a sequence of size `1x1` (B,T), which is a `0` as a single tensor. We observe that the predictions are nonsensical because we have not trained the model yet. Remember, the model is a bigram model, so it predicts the next token only by looking at the previous one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
      "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
      "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
      "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
      "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
      "         40, 64, 16, 52, 62, 13,  1, 25, 57,  3,  9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\""
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predicsitons given a 0 as input\n",
    "preds_int = m.generate(torch.zeros((1, 1), dtype=torch.long), new_tokens=100)\n",
    "print(preds_int)\n",
    "\n",
    "# Decode input\n",
    "decode(preds_int[0].tolist()) # Index 0 as this is the single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram LM Training\n",
    "\n",
    "We can perform the model training in the same way as we have done for `makemore`. We perform the forward pass, set gradients to zero, backpropagate the gradients, and update all parameters' gradients with the Adam optimizer.\n",
    "\n",
    "- Adam: An advanced optimizer. In the previous notebooks, we have used simple stochastic gradient descent (SGD). Adam will update the gradients of all parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch optimiser\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7312, val loss 4.7249\n",
      "step 100: train loss 3.7252, val loss 3.7284\n",
      "step 200: train loss 3.1197, val loss 3.1302\n",
      "step 300: train loss 2.8098, val loss 2.8213\n",
      "step 400: train loss 2.6579, val loss 2.6587\n",
      "step 500: train loss 2.5835, val loss 2.5974\n",
      "step 600: train loss 2.5560, val loss 2.5575\n",
      "step 700: train loss 2.5138, val loss 2.5324\n",
      "step 800: train loss 2.4938, val loss 2.5310\n",
      "step 900: train loss 2.5058, val loss 2.5177\n",
      "step 1000: train loss 2.4912, val loss 2.5029\n",
      "step 1100: train loss 2.4843, val loss 2.5112\n",
      "step 1200: train loss 2.4750, val loss 2.5033\n",
      "step 1300: train loss 2.4689, val loss 2.5012\n",
      "step 1400: train loss 2.4759, val loss 2.5027\n",
      "step 1500: train loss 2.4766, val loss 2.5014\n",
      "step 1600: train loss 2.4856, val loss 2.4995\n",
      "step 1700: train loss 2.4720, val loss 2.4980\n",
      "step 1800: train loss 2.4702, val loss 2.4993\n",
      "step 1900: train loss 2.4682, val loss 2.4949\n",
      "step 2000: train loss 2.4625, val loss 2.4984\n",
      "step 2100: train loss 2.4618, val loss 2.4840\n",
      "step 2200: train loss 2.4734, val loss 2.4912\n",
      "step 2300: train loss 2.4609, val loss 2.4949\n",
      "step 2400: train loss 2.4636, val loss 2.4895\n",
      "step 2500: train loss 2.4612, val loss 2.4975\n",
      "step 2600: train loss 2.4509, val loss 2.4908\n",
      "step 2700: train loss 2.4605, val loss 2.4935\n",
      "step 2800: train loss 2.4675, val loss 2.4749\n",
      "step 2900: train loss 2.4581, val loss 2.4854\n",
      "step 3000: train loss 2.4638, val loss 2.4825\n",
      "step 3100: train loss 2.4733, val loss 2.4873\n",
      "step 3200: train loss 2.4601, val loss 2.4944\n",
      "step 3300: train loss 2.4479, val loss 2.4917\n",
      "step 3400: train loss 2.4491, val loss 2.4836\n",
      "step 3500: train loss 2.4594, val loss 2.4872\n",
      "step 3600: train loss 2.4655, val loss 2.4744\n",
      "step 3700: train loss 2.4541, val loss 2.4838\n",
      "step 3800: train loss 2.4663, val loss 2.4813\n",
      "step 3900: train loss 2.4520, val loss 2.4921\n",
      "step 4000: train loss 2.4587, val loss 2.4823\n",
      "step 4100: train loss 2.4563, val loss 2.4850\n",
      "step 4200: train loss 2.4521, val loss 2.4838\n",
      "step 4300: train loss 2.4545, val loss 2.4792\n",
      "step 4400: train loss 2.4611, val loss 2.4877\n",
      "step 4500: train loss 2.4662, val loss 2.4798\n",
      "step 4600: train loss 2.4573, val loss 2.4798\n",
      "step 4700: train loss 2.4648, val loss 2.4803\n",
      "step 4800: train loss 2.4580, val loss 2.4917\n",
      "step 4900: train loss 2.4651, val loss 2.4832\n",
      "step 5000: train loss 2.4547, val loss 2.4933\n",
      "step 5100: train loss 2.4556, val loss 2.4835\n",
      "step 5200: train loss 2.4717, val loss 2.4830\n",
      "step 5300: train loss 2.4562, val loss 2.4877\n",
      "step 5400: train loss 2.4518, val loss 2.4784\n",
      "step 5500: train loss 2.4448, val loss 2.4874\n",
      "step 5600: train loss 2.4433, val loss 2.4862\n",
      "step 5700: train loss 2.4603, val loss 2.4925\n",
      "step 5800: train loss 2.4575, val loss 2.4964\n",
      "step 5900: train loss 2.4531, val loss 2.4895\n",
      "step 6000: train loss 2.4602, val loss 2.4806\n",
      "step 6100: train loss 2.4506, val loss 2.4935\n",
      "step 6200: train loss 2.4601, val loss 2.4787\n",
      "step 6300: train loss 2.4628, val loss 2.4899\n",
      "step 6400: train loss 2.4601, val loss 2.4724\n",
      "step 6500: train loss 2.4489, val loss 2.4911\n",
      "step 6600: train loss 2.4611, val loss 2.4795\n",
      "step 6700: train loss 2.4545, val loss 2.4802\n",
      "step 6800: train loss 2.4593, val loss 2.4785\n",
      "step 6900: train loss 2.4570, val loss 2.4874\n",
      "step 7000: train loss 2.4577, val loss 2.4984\n",
      "step 7100: train loss 2.4563, val loss 2.4858\n",
      "step 7200: train loss 2.4506, val loss 2.4917\n",
      "step 7300: train loss 2.4618, val loss 2.4862\n",
      "step 7400: train loss 2.4580, val loss 2.4967\n",
      "step 7500: train loss 2.4506, val loss 2.4874\n",
      "step 7600: train loss 2.4561, val loss 2.4758\n",
      "step 7700: train loss 2.4534, val loss 2.4769\n",
      "step 7800: train loss 2.4600, val loss 2.4842\n",
      "step 7900: train loss 2.4574, val loss 2.4887\n",
      "step 8000: train loss 2.4514, val loss 2.4836\n",
      "step 8100: train loss 2.4524, val loss 2.4768\n",
      "step 8200: train loss 2.4555, val loss 2.4903\n",
      "step 8300: train loss 2.4540, val loss 2.4955\n",
      "step 8400: train loss 2.4592, val loss 2.4921\n",
      "step 8500: train loss 2.4507, val loss 2.4877\n",
      "step 8600: train loss 2.4595, val loss 2.4812\n",
      "step 8700: train loss 2.4564, val loss 2.4843\n",
      "step 8800: train loss 2.4537, val loss 2.4826\n",
      "step 8900: train loss 2.4620, val loss 2.4860\n",
      "step 9000: train loss 2.4517, val loss 2.4821\n",
      "step 9100: train loss 2.4544, val loss 2.4811\n",
      "step 9200: train loss 2.4570, val loss 2.4912\n",
      "step 9300: train loss 2.4508, val loss 2.4810\n",
      "step 9400: train loss 2.4564, val loss 2.4901\n",
      "step 9500: train loss 2.4598, val loss 2.4860\n",
      "step 9600: train loss 2.4588, val loss 2.4899\n",
      "step 9700: train loss 2.4645, val loss 2.4827\n",
      "step 9800: train loss 2.4622, val loss 2.4837\n",
      "step 9900: train loss 2.4501, val loss 2.4906\n",
      "2.53613018989563\n"
     ]
    }
   ],
   "source": [
    "# Training the bigram model\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # Print train and validation loss for eval_iters\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Get a batch\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # Set gradients to 0\n",
    "    optimizer.zero_grad(set_to_none=True) # more efficient\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "\n",
    "    # Adam, update gradients\n",
    "    optimizer.step()\n",
    "\n",
    "# Print final loss (batch loss)\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HAs; n og ARKEve tous oone nt, thintis n Thimed;\n",
      "Houchy me t ckin by mapa, owis.\n",
      "Andeaneshe dva\n",
      "Age f:\n",
      "Yome as my rercad locheres.\n",
      "I oly t t onddocint brss be ceed vemmy I ngowhinothabld\n",
      "tat hel I ompatyor yo t.\n",
      "IIORIDe s, ir Bewilf sof ate k susey s t usprgr!\n",
      "TUKiler igerindsolle CESIOFormegine s f\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions given a 0 as input and decode input (in one line)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), new_tokens=300)[0].tolist())) # Index 0 as this is the single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a considerable improvement after the training stage. However, the bigram model is severly limited as it considers just one previous token. Next we introduce the core of the transformer: self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mathematical Trick in Self-Attention\n",
    "\n",
    "We are given three dimensions `(B,T,C)` where B is the batch, T is time, and C is the Channel, representing some information about the token. In the bigram LM, the tokens do not pay attention to each other. \n",
    "We want to change this now. What needs to be changed? We want the characters in the _time_ dimension to communicate with each other, meaning tokens of the same sequence should exchange information stored in the channels.\n",
    "\n",
    "__Introduction: Average Post Context Channels__\n",
    "\n",
    "The way tokens attend to each other is by considering only the preceding tokens, not the subsequent ones, as we aim to predict those. The simplest way to do this is to take the average of the channels (which store the tokens' information) of the preceding tokens for a given token. For example, if we are at position 3, we want the average of the channels of tokens 3, 2, and 1. The resulting vector becomes a feature vector of position 3 that summarizes the information at this and the previous positions. However, averaging information, of course, loses some important information along the way; we will later see how this can be improved.\n",
    "\n",
    "First, within a batch, for every token of T, we want to calculate the average of the channels of preceding tokens.\n",
    "\n",
    "__Note__ the design of the 3D tensor. \n",
    "\n",
    "- Batch: Each batch is a layer of the tensor, which are stacked on each other. \n",
    "\n",
    "- Time: Within a layer, the rows correspond to time.\n",
    "\n",
    "- Channel: Within a layer, the columns correspond to the channel.\n",
    "\n",
    "We can think of a `(4,8,2)` tensor as 4 matrices each with 8 rows and 2 columns stacked on top of each other to form a 3D array.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Batch 1:\n",
    "  Time 1: [Channel1_value, Channel2_value, Channel3_value]\n",
    "  Time 2: [Channel1_value, Channel2_value, Channel3_value]\n",
    "  Time 3: [Channel1_value, Channel2_value, Channel3_value]\n",
    "  Time 4: [Channel1_value, Channel2_value, Channel3_value]\n",
    "\n",
    "Batch 2:\n",
    "  Time 1: [Channel1_value, Channel2_value, Channel3_value]\n",
    "  Time 2: [Channel1_value, Channel2_value, Channel3_value]\n",
    "  Time 3: [Channel1_value, Channel2_value, Channel3_value]\n",
    "  Time 4: [Channel1_value, Channel2_value, Channel3_value]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.2858,  0.9651],\n",
       "         [-2.0371,  0.4931],\n",
       "         [ 1.4870,  0.5910],\n",
       "         [ 0.1260, -1.5627],\n",
       "         [-1.1601, -0.3348],\n",
       "         [ 0.4478, -0.8016],\n",
       "         [ 1.5236,  2.5086]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 1.0101,  0.1215],\n",
       "         [ 0.1584,  1.1340],\n",
       "         [-1.1539, -0.2984],\n",
       "         [-0.5075, -0.9239],\n",
       "         [ 0.5467, -1.4948],\n",
       "         [-1.2057,  0.5718],\n",
       "         [-0.5974, -0.6937]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.3514, -0.2759],\n",
       "         [-1.5108,  2.1048],\n",
       "         [ 2.7630, -1.7465],\n",
       "         [ 1.4516, -1.5103],\n",
       "         [ 0.8212, -0.2115],\n",
       "         [ 0.7789,  1.5333],\n",
       "         [ 1.6097, -0.4032]]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For replicability\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Create a tensor of this dimension\n",
    "B,T,C=4,8,2\n",
    "\n",
    "x=torch.randn(B,T, C)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version 1: Looping__\n",
    "\n",
    "Within a batch, for each `t`, we can now compute the average of channles for all precedding tokens of `t`, including `t`. We thus average out the time dimension. Remember that a channel is an embedding in this case which stores the information about the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor of 0s\n",
    "xbow=torch.zeros((B,T,C))\n",
    "\n",
    "# For each batch, for each time (row), get the mean of the channels previous to t, for both channels\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev=x[b,:t+1] # returns (t,C)\n",
    "        xbow[b,t,:]=xprev.mean(dim=0) # replaces 0s with column mean\n",
    "\n",
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version 2: Matrix Multiplication__\n",
    "\n",
    "Here comes the trick. The looping approach described above is quite inefficient. Instead, we can leverage the more efficient matrix multiplication. Let's use a toy example to illustrate the inner workings. Suppose we have two tensors `a` and `b` with dimensions 3x3 and 3x2, respectively. Multiplying both gives a new matrix `c` with dimensions 3x2. We use the `torch.tril()` function, which returns the lower triangle of a matrix. If we input a matrix with ones for `a`, and multiply it with `b`, we get the following `c`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialise tensors\n",
    "a=torch.tril(torch.ones(3,3))\n",
    "b=torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Matrix multiplication\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `c`'s columns are now a \"moving addition\" of the the columns of tensor `b`. This is flexible to any number of rows. Now, to get the averages, as we currently do sums, we can adjust the diagonal torch of ones to a matrix with weights for computing the average! This can be done by dividing each row by its row sum, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialise tensors\n",
    "a=torch.tril(torch.ones(3,3))\n",
    "a = a / a.sum(1, keepdim=True) # This gets the weights for the average\n",
    "b=torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Matrix multiplication\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor `a` is now a matrix of weights, and `c`'s columns are now \"moving averages\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the logic from the toy example to the `x` tensor. We can multiply a weights matrix times `x`. The weight matrix is `(T,T)` and the `x` is (B,T,C). Multiplication gives `(B, T,T) x (B,T,C)` as broadcasting adds the batch dimension for the weights matrix. The mutliplication is for each batch, B, in parallel. Thus the first dimension is unchaned. The second dimension is then `(T,T)x(T,C)` and results in dimension `TxC` for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight matrix\n",
    "wei=torch.tril(torch.ones(T,T)) # dim (T,T) \n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform matrix multiplication\n",
    "xbow2 = wei @ x # dim (B,T,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix mutliplication yields the same result as the loop, but is more efficient as computations can be performed in parallel. The gist is that we do matrix multiplication, where the aggregation is perfromed with weights that lead to the average channels of the precedding tokens of `t`. The matrix mutliplication essentially calculates weighted sums. This weighted sum of the channles preceeding `t` contain information of how important those are for `t`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version 3: Using sofmax__\n",
    "\n",
    "This is identical to the both other versions, but more NLP-like as we use functons we also use in NNs. The idea is as follows:\n",
    "\n",
    "- `tril` is a `TxT` of ones\n",
    "- `wei` is initialised with 0. Its upper diagonal is replaced with negative infinity. Applying softmax rowwise first takes the exponential of each value and standardises those. All minus infinities are zero due to exponentialisation, and all 0s become 1. Normalising yields the weights for column-wise aggreation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\")) # replace 0s from tril with -inf\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "xbow3= wei @ x\n",
    "xbow3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance weights!__\n",
    "\n",
    "The weights matrix captures how important the preceeding tokens are for token `t`. For each preceeding token we have a weight of attention for `t` which captures how much `t`'s channel should be influenced by this token.\n",
    "\n",
    "As a preview: the weights are data-dependent and learnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Intermediate summary__\n",
    "\n",
    "We can weighted aggregations of past okens by using matrix mutliplication of a low-trianglualr fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention: A single-head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current code above, we take the average of the preceding tokens' embeddings with _equal_ weights. The __crux__ of self-attention is that these weights are learned. For a given token `t`, not all preceding tokens are equally important, hence the weights are learned in a data-dependent way.\n",
    "\n",
    "__Attention__\n",
    "\n",
    "Self-attention learns the weights through query-key vectors for each token. The _query_ vector asks, \"What am I looking for?\" The _key_ vector answers, \"What do I contain?\" To learn attention between tokens, we take the dot product of the keys and queries. For example, token `t`'s query takes the dot product with all the keys of the preceding tokens, and those dot products become the weights of the weight matrix. The weight matrix is a `(T,T)` matrix, so for each row, the columns give the weights for the other, preceding-only, tokens.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "           t1    t2    t3    t4  t5\n",
    "t1   [ 0.1   0.1     0     0     0 ]\n",
    "t2   [ 0.1   0.2   0.1     0     0 ]\n",
    "t3   [ 0.1   0.2   0.3   0.1     0 ]\n",
    "t4   [ 0.1   0.2   0.3   0.4   0.1 ]\n",
    "t5   [ 0.1   0.2   0.3   0.4   0.5 ]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The intuition is that the higher the dot product of key and query, the more token `t` should pay attention to the token of the query. This is due to the higher weight in the matrix multiplication that results in the vector for `t`.\n",
    "\n",
    "__Query and Key Vectors__\n",
    "\n",
    "For each token, we create keys and queries by inputting the token embeddings into a linear layer. That is, `x` `(B,T,C)` into, e.g., a key layer of dimension `(B, C, head_size)`, resulting in a key dimension of `(B,T,head_size)`. For each token, we now have a key embedding. The same process applies for the query. The linear layer transforms the channels of the input tensor into a space defined by the `head_size`. The layers' weights find the optimal way to transform the input vectors into this new space, which is used for attention through query and key vectors.\n",
    "\n",
    "_Why isn't there a lookup table as for embeddings?_ Short answer: embeddings are used to convert data into numerical vector representations. Here, we feed in vector inputs, so we can work with them directly.\n",
    "\n",
    "__Weights__\n",
    "\n",
    "To get the weights, we have to multiply keys and queries for all tokens, i.e., `keys @ queries` `(B,T,head_size) x (B,T,head_size)`. But as both have the same dimensions, we have to transpose the latter, so that we multiply the dimensions `(B,T,head_size) x (B,head_size,T)`, resulting in a weight matrix of dimensions `(B,T,T)` (intuitive, as we need these dimensions so that each token can look at each other). This is simply performing the dot products for all keys and queries. We then set the upper diagonal to zero, as we do not allow subsequent attention, and apply softmax row-wise.\n",
    "\n",
    "__Value Vectors__\n",
    "\n",
    "In addition to query and key vectors, each token has a value vector. This represents the value of the token and is distinct from the embedding vector. It is the information provided when other tokens find this token useful. So instead of aggregating the information in channels, the token embeddings, we aggregate the value vectors that are specific to the attention head. The value vectors are of the same dimension as query and key, and the final output of the attention head is then `(B,T,T) x (B,T,head_size)` which is `(B,T,head_size)`.\n",
    "\n",
    "__This is the crux: the weights are learned through self-attention of tokens.__\n",
    "\n",
    "- Head: a single, __independent__ head computes attention scores and generates output vectors of the input sequence.\n",
    "\n",
    "- Multi-head: combines multiple independent heads that can focus on different representation subspaces and learn different weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0291, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0418, 0.0425, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2255, 0.1838, 0.1734, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0264, 0.4293, 0.0568, 0.0452, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6365, 0.0698, 0.0441, 0.0012, 0.4765, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0115, 0.1043, 0.0586, 0.7471, 0.0076, 0.9833, 0.0000, 0.0000],\n",
       "         [0.0208, 0.0796, 0.4679, 0.0859, 0.3022, 0.0120, 0.7871, 0.0000],\n",
       "         [0.0085, 0.0907, 0.1991, 0.1207, 0.2136, 0.0046, 0.2129, 1.0000]],\n",
       "\n",
       "        [[0.0032, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4790, 0.1467, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1938, 0.0325, 0.5137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1158, 0.4659, 0.2962, 0.3892, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0137, 0.0415, 0.0149, 0.0983, 0.1711, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0614, 0.1937, 0.0017, 0.1796, 0.5229, 0.4874, 0.0000, 0.0000],\n",
       "         [0.1045, 0.0344, 0.1588, 0.2669, 0.0429, 0.0154, 0.5976, 0.0000],\n",
       "         [0.0285, 0.0852, 0.0146, 0.0660, 0.2632, 0.4972, 0.4024, 1.0000]],\n",
       "\n",
       "        [[0.0204, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1670, 0.1685, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1496, 0.2347, 0.0602, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0766, 0.0316, 0.2469, 0.3222, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0331, 0.2305, 0.0100, 0.1418, 0.3852, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0318, 0.1239, 0.0991, 0.2950, 0.1011, 0.7209, 0.0000, 0.0000],\n",
       "         [0.3526, 0.1302, 0.2338, 0.1846, 0.3886, 0.0798, 0.9101, 0.0000],\n",
       "         [0.1689, 0.0807, 0.3500, 0.0564, 0.1252, 0.1993, 0.0899, 1.0000]],\n",
       "\n",
       "        [[0.0848, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0569, 0.0672, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1074, 0.5844, 0.0854, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1497, 0.0840, 0.0038, 0.1597, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0899, 0.0688, 0.6711, 0.2166, 0.5119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1969, 0.0430, 0.1159, 0.3517, 0.0586, 0.3612, 0.0000, 0.0000],\n",
       "         [0.0475, 0.1337, 0.0310, 0.1158, 0.3429, 0.0134, 0.9435, 0.0000],\n",
       "         [0.2668, 0.0189, 0.0928, 0.1561, 0.0867, 0.6254, 0.0565, 1.0000]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: self-attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C=4,8,32\n",
    "x=torch.randn(B,T, C)\n",
    "\n",
    "# HEAD\n",
    "head_size = 16 # hyperparameter\n",
    "key=nn.Linear(C, head_size, bias=False) # Layer to get key vectors\n",
    "query=nn.Linear(C, head_size, bias=False) # Layer to get query vectors\n",
    "query=nn.Linear(C, head_size, bias=False) # Layer to get value vectors\n",
    "\n",
    "# Communication: dot-product\n",
    "k = key(x) # (B,T,16) get all keys\n",
    "q = query(x) # (B,T,16) get all queries\n",
    "v = query(x) # (B,T,16) get all values\n",
    "wei = q @ k.transpose(-2, -1) # (B, T,T) Needs to be transposed as bothe have same dims\n",
    "\n",
    "# Remove upper diagonal\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T)) # do not need this anymore as defined and learned\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\")) \n",
    "wei = F.softmax(wei, dim=1) # Softmax for normalising dot-products\n",
    "\n",
    "out= wei @ v * head_size**-.5\n",
    "wei\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can eight `(T,T)` weight matrixes. Each weight in those matrixes is data-dependendly learnedd through the self-attention mechanism. The weigths literally tell us how much of the information from each preceeding token for a given token `t` has to be aggregated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Self-Attention\n",
    "\n",
    "1. Attention is a communication mechanism among tokens.\n",
    "\n",
    "2. Attention per se knows nothing about the position of tokens. That is why we add positional encodings to the token embeddings.\n",
    "\n",
    "3. Attention operates within dimension T, not across. That is, tokens within a context window communicate with each other.\n",
    "\n",
    "4. In _encoder_ models, where the goal is not to generate text but to produce encodings of the text, the full weight matrix is of interest, meaning there is no masking of subsequent tokens. In _decoder_ models, where the goal is to generate text, the weight matrix is masked to prevent future information from being used.\n",
    "\n",
    "5. The term \"self-attention\" comes from the fact that queries, keys, and values are retrieved from the same source input embeddings. This is in contrast to \"cross-attention,\" where these elements are retrieved from different sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the original paper, attention is defined as follows:\n",
    "\n",
    "![images/attention_formula.png](images/attention_formula.png)\n",
    "\n",
    "which is precisely what we are doing. We multiply query and key to get the weight matrix, which we finally multiply with the value. This is also called scaled dot-product attention. The scaling is the normalization by the square root of the head dimension. Why is this important?\n",
    "\n",
    "When Q and K have unit variance, multiplying them does not yield a resulting matrix with unit variance. Scaling achieves this and maintains the unit variance. This is critical to avoid saturating the softmax. Saturation occurs when extreme values are passed through softmax, resulting in unchanging values and minimal gradients, which can hinder learning.\n",
    "\n",
    "We can now create the head as a class. The attention head is typically located before a layer. Typically, the attention vectors do not have a bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention: Multi-Head\n",
    "\n",
    "Multi-head attention is combining mutliple independent attention heads and concatenating there inputs. The efficiency here is that they can be learned in parallel.\n",
    "\n",
    "![images/mulit_head.png](images/multi_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model\n",
    "\n",
    "We implement the following other modules from the original paper:\n",
    "\n",
    "![images/archit.png](images/archit.png)\n",
    "\n",
    "- Multi-Head: Here, we divide the embedding size by the number of heads, so that when we concatenate the output vectors, we get a vector of the original embedding size. In our case, the embedding size is `32`. We have four heads, each with a head embedding size of `8`. So, concatenating gives back a `32` size vector which is fed into the linear layer.\n",
    "\n",
    "- Feedforward: This is a simple MLP (Multi-Layer Perceptron). It inputs the output of the multi-head attention and performs the \"thinking\". We can think of the attention mechanism as communication among the tokens, while the feedforward pass is the \"thinking\" process between tokens. Note that the tokens in this layer \"think\" independently of each other. This is effective because their embeddings have been enriched with communication from the attention mechanism.\n",
    "\n",
    "- Blocks: The transformer consists of repeated blocks that themselves consist of communication and thinking. There is cross attention from the encoder model in the paper, which we won't implement. A block first performs multi-head self-attention and then the feedforwarding of the resulting embeddings.\n",
    "\n",
    "- Dropout: This is a regularization mechanism. We can add this right before the residual connection. This mechanism randomly zeroes a fraction of neurons and thus helps in avoiding overfitting. Every forward and backward pass, a different subset of neurons is shut down. In this sense, we can think of adding dropout as learning an ensemble of neural nets. At inference time, all these sub-networks are merged into a single neural network. Hence, this can be seen as an ensemble method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization: Residual Connection and Layer Norm\n",
    "\n",
    "As we see below, we already have a deep neural network. Deep neural networks run the risk of lower optimizability due to issues like gradient vanishing. This means that gradients do not flow effectively through the network. This issue can arise from saturated activation functions, such as tanh, that have tiny gradients at their extremities. This results in smaller gradients for earlier layers, inhibiting their learning and leading to poor optimization.\n",
    "\n",
    "There are two solutions: 1) Residual Connections and 2) Layer Norms.\n",
    "\n",
    "1. **Residual Connections**: These are the arrows and \"Add\" that skip the blocks above. The basic idea is that you transform the data with layers and add a skip connection by adding the previous features. Their purpose is to allow gradients to flow more directly through the network. This is achieved through the summation, which has a gradient of 1 and thus helps in backpropagating a larger gradient to earlier layers. For instance, if we have \\( y = f(x) + x \\), which is a residual connection, the gradient of the weights on y is:\n",
    "\n",
    "   $$ \\frac{dy}{dw} = \\frac{dy}{dx} \\cdot \\frac{dx}{dw} $$\n",
    "\n",
    "   With a residual connection, this becomes:\n",
    "\n",
    "   $$ \\frac{dy}{dw} = \\left( \\frac{df}{dx} + 1 \\right) \\cdot \\frac{dx}{dw} $$\n",
    "\n",
    "   Without it:\n",
    "\n",
    "   $$ \\frac{dy}{dw} = \\frac{df}{dx} \\cdot \\frac{dx}{dw} $$\n",
    "\n",
    "   The addition of x thus increases the gradients for the weights and optimizes learning. Projection layers often appear when the transformation of x results in a different dimension than the input dimension.\n",
    "\n",
    "   Residual connections can be visualized as adding `x` itself to the output of the block `F(x)`, providing an alternative, more direct pathway for gradient flow. This direct connection also gives the network an identity function, where it can just learn the function \\( F(x) \\) if no further transformations are necessary.\n",
    "\n",
    "   ![images/res_connection.png](images/res_connection.png)\n",
    "\n",
    "2. **Layer Normalization**: The idea is to normalize the samples, T, across the channels, C. For a (T,C) layer, normalization normalizes the rows, computing the mean and standard deviation for each T, and normalizes the channel values. It normalizes the output of all neurons in a layer for this T to have a Gaussian distribution. Two additional parameters, gamma and beta, are learnable to adjust the distribution if the optimal normalization is not Gaussian.\n",
    "\n",
    "   The transformer architecture has evolved, with the position of the layer norm now commonly implemented before a block.\n",
    "\n",
    "   ![images/layer_norm.png](images/layer_norm.png)\n",
    "\n",
    "__Difference to Batch Normalization__\n",
    "\n",
    "Batch normalization and layer normalization differ primarily in the axis along which normalization is applied. Batch normalization normalizes across the batch dimension, while layer normalization does so across the channels.\n",
    "\n",
    "   ![images/layer_batch_norm.png](images/layer_batch_norm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note C is the embeddin dims__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "We have encoded tokens as integers. Additionally, we can encode their positions to provide the model with information about the sequence. We can include a second table, a position table, in a similar manner to the embedding table. Each position in the block size receives its __own__ embedding. That is, each of the positions in the block size gets a unique embedding that captures information about the token's position in the context window.\n",
    "\n",
    "We integrate the position embedding by adding it to the token embeddings. This works because both are embedded into the same dimensions. Thus, instead of feeding only the token embeddings into the first layer, we feed a summation of token and position embeddings.\n",
    "\n",
    "Therefore, `x` contains information about both the token and its position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size=32\n",
    "block_size=16\n",
    "max_iters = 5000\n",
    "eval_iters=500\n",
    "eval_interval=100\n",
    "learning_rate=1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_embed=64\n",
    "n_head = 6\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "\n",
    "# Seed for repl.\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Single Attention Head\n",
    "class Head(nn.Module):\n",
    "    '''Class for a self-attention head.'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create query, key, and value vectors (no biases)\n",
    "        self.key=nn.Linear(n_embed, head_size, bias=False) # (C, head_size)\n",
    "        self.query=nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value=nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        #? check what this is?\n",
    "        self.register_buffer('tril', torch.ones(block_size, block_size))\n",
    "\n",
    "        # Drop out layer for regularisation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get dims of input data (B,T,C)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Get keys, queries, value vectors; (B,T,C) x (C,head_size) => (B,T,head_size) \n",
    "        k = self.key(x) # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "        v = self.value(x) # (B,T,head_size)\n",
    "\n",
    "        # Compute attention scores, i.e. masked weights\n",
    "        # qxk attention scores: q (B,T,head_size) x k.t (B,head_size,T) => (B,T,T) (this is our familiar weight matrix)\n",
    "        wei = q @ k.transpose(-2, -1) *k.shape[-1]**-.5 # Scaling to avoid saturation\n",
    "        # Mask upper diag + scaling\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('inf')) # Mask diagonal, which makes it a decoder\n",
    "        # Apply softmax to get probs\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        # Dropout layer: randomly zeroes weights\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Attentiosn scores times value\n",
    "        out = wei @ v # (B,T,T) x (B,T,head_size) => (B,T,head_size)\n",
    "        return out \n",
    "    \n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''mutliple attention heads in parallel''' \n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Get a list of attention heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "        # Projection layer for the same dim for residual connection\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Insert x in all heads and get scores in parallel and indep.\n",
    "        out = [h(x) for h in self.heads] # list of (B,T, head_size)\n",
    "        \n",
    "        # Append out vectors from all heads\n",
    "        # Gives (B,T, head_size*num_heads) = (B,T,n_embed) (in our case)\n",
    "        out = torch.cat(out, dim=-1)\n",
    "\n",
    "        # project on new layer for resid. connection\n",
    "        out = self.proj(out) # (B,C,C)\n",
    "\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Feedforward MLP\n",
    "class FeedForward(nn.Module):\n",
    "    '''singe linear layer with non-linear activation'''\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net=nn.Sequential(nn.Linear(n_embed, n_embed * 4), # (B, C, C*4); times for as i the OG paper\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(n_embed * 4, n_embed), #(B, C*4, C)  projection layer\n",
    "                               nn.Dropout(dropout), # Dropout layer\n",
    "                              )\n",
    "   \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# Layer norm (won't use this but PyTorch's module instead)\n",
    "class LayerNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5):\n",
    "    # For numeric stability\n",
    "    self.eps = eps\n",
    "\n",
    "    # Initialise gamma and beta for distribution adjustment\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # FORWARD\n",
    "    # Get mean and std by row\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "\n",
    "    # Apply normalisation\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''communication followed by thinking; this is a \"layer\"'''\n",
    "    \n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        # Head size is embd size/n_head so that concat yields embd size\n",
    "        head_size= n_embed // n_head\n",
    "\n",
    "        # Define lements: multi-head, feedforward, layer norms\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        # Pre-multi-head\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        # Pre-feedforward\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward with residual connections\n",
    "        # RC: direct path from x->x and transformation path x->F(x)\n",
    "        x = x + self.sa(self.ln1(x)) # + x is residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build Bigram LM\n",
    "class GPTLanguageModel(nn.Module): # inherent from the nn class\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create embedding matrix\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # This is a 65 x n_embed matrix; weights are inintialised at random\n",
    "        \n",
    "        # Create position matrix (same dim as token embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed) # This is a 65 x n_embed matrix; weights are inintialised at random\n",
    "\n",
    "        # Transformer blocks; n_layer = number of blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # Final layer norm before logits\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "\n",
    "        # Create linear input layer to get logits\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # Get input data dims\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Index rows to get the embeddings of tokes\n",
    "        # idx (B,T) indexes (C,C) => (B,T,C)\n",
    "        tok_emb = self.token_embedding_table(idx) # dim (B,T,C)\n",
    "\n",
    "        # Index all rows of the position emb, dim T (context window)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        # Combine token and position embeddings\n",
    "        x = tok_emb + pos_emb # dim (B,T,C)\n",
    "\n",
    "        # Pass x through all n_layer blocks sequentially\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "\n",
    "        # Output through Layer Norm before logits\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "\n",
    "        # Pass through linear layer to get logits\n",
    "        logits = self.lm_head(x) # dim (B,T, vocab_size)\n",
    "\n",
    "        # Loss function\n",
    "        if targets is None: # This is for eval function\n",
    "            loss=None\n",
    "        else:\n",
    "            # Reshape logits and targets for loss function\n",
    "            B,T,C=logits.shape # dims\n",
    "            logits =logits.view(B*T,C) \n",
    "            targets =targets.view(B*T) \n",
    "        \n",
    "            # Compute loss\n",
    "            loss=F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, new_tokens):\n",
    "        '''Generate new tokens from an input sequence. \n",
    "        As this is a bigram model, only the last Time dim is considered'''\n",
    "\n",
    "        for _ in range(new_tokens):\n",
    "\n",
    "            # Crop index to not be larger than context window\n",
    "            idx_cond = idx[:, -block_size:] # All rows, but not all coloumns\n",
    "\n",
    "            # Get the logits  \n",
    "            logits, loss = self(idx_cond) # loss is ignored, no use here\n",
    "\n",
    "            # Consider last time dim only; makes (B,T,C) to (B,C) \n",
    "            logits = logits[:,-1,:] # picks last T, becomes (B,C), bc we want the latest char\n",
    "            \n",
    "            # Convert logits into probs\n",
    "            probs = F.softmax(logits, dim=-1) # Row sum=1 \n",
    "\n",
    "            # Sample from each rwo\n",
    "            idx_next=torch.multinomial(probs, num_samples=1) # (B,1), as we only get one prediction\n",
    "\n",
    "            # Append idx_next to idx\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # Append across dim 1; (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.204609 M parameters\n",
      "step 0: train loss 4.3469, val loss 4.3480\n",
      "step 100: train loss 1.6583, val loss 1.7003\n",
      "step 200: train loss 0.3396, val loss 0.3672\n",
      "step 300: train loss 0.2142, val loss 0.2221\n",
      "step 400: train loss 0.1823, val loss 0.1826\n",
      "step 500: train loss 0.1700, val loss 0.1727\n",
      "step 600: train loss 0.1642, val loss 0.1655\n",
      "step 700: train loss 0.1606, val loss 0.1629\n",
      "step 800: train loss 0.1586, val loss 0.1601\n",
      "step 900: train loss 0.1563, val loss 0.1567\n",
      "step 1000: train loss 0.1553, val loss 0.1556\n",
      "step 1100: train loss 0.1509, val loss 0.1546\n",
      "step 1200: train loss 0.1526, val loss 0.1546\n",
      "step 1300: train loss 0.1498, val loss 0.1521\n",
      "step 1400: train loss 0.1474, val loss 0.1501\n",
      "step 1500: train loss 0.1445, val loss 0.1467\n",
      "step 1600: train loss 0.1457, val loss 0.1470\n",
      "step 1700: train loss 0.1433, val loss 0.1437\n",
      "step 1800: train loss 0.1430, val loss 0.1445\n",
      "step 1900: train loss 0.1426, val loss 0.1437\n",
      "step 2000: train loss 0.1431, val loss 0.1440\n",
      "step 2100: train loss 0.1410, val loss 0.1426\n",
      "step 2200: train loss 0.1393, val loss 0.1414\n",
      "step 2300: train loss 0.1418, val loss 0.1424\n",
      "step 2400: train loss 0.1397, val loss 0.1411\n",
      "step 2500: train loss 0.1384, val loss 0.1398\n",
      "step 2600: train loss 0.1383, val loss 0.1398\n",
      "step 2700: train loss 0.1392, val loss 0.1417\n",
      "step 2800: train loss 0.1371, val loss 0.1377\n",
      "step 2900: train loss 0.1354, val loss 0.1386\n",
      "step 3000: train loss 0.1364, val loss 0.1397\n",
      "step 3100: train loss 0.1354, val loss 0.1390\n",
      "step 3200: train loss 0.1365, val loss 0.1374\n",
      "step 3300: train loss 0.1350, val loss 0.1379\n",
      "step 3400: train loss 0.1360, val loss 0.1392\n",
      "step 3500: train loss 0.1355, val loss 0.1374\n",
      "step 3600: train loss 0.1350, val loss 0.1358\n",
      "step 3700: train loss 0.1332, val loss 0.1375\n",
      "step 3800: train loss 0.1330, val loss 0.1352\n",
      "step 3900: train loss 0.1333, val loss 0.1346\n",
      "step 4000: train loss 0.1347, val loss 0.1347\n",
      "step 4100: train loss 0.1321, val loss 0.1344\n",
      "step 4200: train loss 0.1321, val loss 0.1363\n",
      "step 4300: train loss 0.1320, val loss 0.1359\n",
      "step 4400: train loss 0.1301, val loss 0.1334\n",
      "step 4500: train loss 0.1298, val loss 0.1331\n",
      "step 4600: train loss 0.1310, val loss 0.1342\n",
      "step 4700: train loss 0.1313, val loss 0.1349\n",
      "step 4800: train loss 0.1302, val loss 0.1339\n",
      "step 4900: train loss 0.1307, val loss 0.1344\n",
      "0.12831823527812958\n"
     ]
    }
   ],
   "source": [
    "# Model and\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "# PyTorch optimiser\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iters):\n",
    "\n",
    "    if step % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Get a batch\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Set gradients to 0\n",
    "    optimizer.zero_grad(set_to_none=True) # more efficient\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "\n",
    "    # Adam, update gradients\n",
    "    optimizer.step()\n",
    "\n",
    "# Print final loss (batch loss)\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.1307), 'val': tensor(0.1344)}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final (batch) loss\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ME\n",
      "NG CUS:\n",
      "YCO:\n",
      "shearloar 'st, pithour hor bace\n",
      "Fortich hy starrid.\n",
      "If here sar sauriine sie if of\n",
      "chor hath fash is starm soart he late,\n",
      "Pholl it hapry;\n",
      "'s you h thre me,\n",
      "Dy\n",
      "Homn om batis, that oh this me with\n",
      "\n",
      "Whos, of ieaque antish izany:\n",
      "Bus, h\n"
     ]
    }
   ],
   "source": [
    "# Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, new_tokens=250)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final output, I have scaled up the language model slightly to include larger embeddings, among other adjustments. The final output is still somewhat noisy, but we can observe that the model produces a reasonably good output. It's important to note that we are still operating on a character level and the model is under-scaled. The key takeaway is the significant leap from the Bigram Model to the GPT model, achieved primarily through the implementation of the transformer architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the Architecture\n",
    "\n",
    "What we implemented is a _decoder-only_ transformer, which corresponds to the right-hand side of the transformer architecture image above. It is characterized as a decoder because we use triangular masking in the attention mechanism, preventing the model from learning from future tokens of a token `t`. The decoder is primarily used for generating text. An encoder, represented by the left-hand side architecture in the transformer image, is used for encoding tokens into embeddings and is utilized for downstream ML tasks, such as classification.\n",
    "\n",
    "The original concept of this architecture was for translation. The encoder would encode a language (e.g., Italian) and the decoder would then translate it into another language (e.g., German). The idea is to condition the decoder on the information learned from the encoder. However, what we have done is slightly different; we did not condition but simply fed our training data into the decoder.\n",
    "\n",
    "The encoder differs as it does not apply the triangular masking. The encoder thus learns the representation of the inputted sentence in Italian and aims to understand the content of this sentence. This contrasts with the decoder, which focuses on predicting the next token.\n",
    "\n",
    "The final embeddings from the encoder are then fed as keys and queries into the decoder, in a \"cross-attention\" mechanism. In an attention head, the _keys_ and _values_ come from the encoder block, matched with the _queries_ from the decoder.\n",
    "\n",
    "## Notes on GPT\n",
    "\n",
    "How does GPT work? There are two stages: the pre-training phase and the fine-tuning stage. The training stage is similar to our implementation above, where the decoder model learns to generate text from a large corpus. After the pre-training stage, the decoder can complete various tasks it has seen during training, but it is not yet the question-answer bot we know.\n",
    "\n",
    "This capability is developed in the fine-tuning stage, which involves three phases. First, data in the desired question-answer format is collected for fine-tuning the model. Second, a reward model is trained, where GPT outputs several answers to a question and humans rank these answers. In the third stage, they optimize a policy with respect to the reward model, aiming for GPT to output an answer that yields a high reward (more details on this at a later stage).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
